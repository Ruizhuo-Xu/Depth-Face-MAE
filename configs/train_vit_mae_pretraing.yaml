anno_file: ./data/lock3dface_and_bfm_large.json
max_epochs: 20
lr: 6.e-4
exp_name: vit_mae_tiny_pretraining_lock3d_and_bfm_v4
devices: [3]

dataset:
  train_set:
    name: ImgDataset
    args: 
      anno_file: ${anno_file}
      split: train
      # first_k: 10000
  val_set:
    name: ImgDataset
    args:
      anno_file: ${anno_file}
      split: val
      test_mode: True
  batch_size: 1024
  num_workers: 64

model:
  name: ModelForMAE
  args:
    mask_ratio: 0.75
    model_spec:
      name: MAE-ViT
      args:
        img_size: 128
        patch_size: 8
        in_chans: 1
        embed_dim: 384
        depth: 8
        num_heads: 8
        mlp_ratio: 4
        decoder_embed_dim: 256
        decoder_depth: 4
        decoder_num_heads: 8
        norm_pix_loss: True
        pred_normal_map: True
        drop_path: 0.0
        id_loss_args:
          feat_dim: 128
          dropout: 0.0
          num_classes: ${model.args.num_classes}
          arcface_args:
            in_features: ${model.args.model_spec.args.id_loss_args.feat_dim}
            out_features: ${model.args.num_classes}
            s: 30.0
            m: 0.0
    optimizer_spec:
      name: adamw
      args: 
        lr: ${lr}
        weight_decay: 0.05
        betas: [0.9, 0.95]
    lr_sched_spec:
      name: CosineDecayWithWarmup
      args:
        warmup_fraction: 0.05
        max_epochs: ${max_epochs}
        lr: ${lr}
        min_lr: 1.e-9
    num_classes: 10600
    validation_on_gallery: True
    is_lock3dface: True
    anno_file: ${anno_file}

trainer:
  wandb_logger:
    name: ${exp_name}
    project: Depth-Face-MAE
  checkpoint:
    every_n_epochs: 5
    dirpath: /home/mnt/rz_mnt/depth-face-mae/checkpoints/pretraining
    # filename: "{epoch:02d}-{global_step}"
    save_top_k: -1
  args:
    devices: ${devices}
    max_epochs: ${max_epochs}
    log_every_n_steps: 10
    precision: 16-mixed
